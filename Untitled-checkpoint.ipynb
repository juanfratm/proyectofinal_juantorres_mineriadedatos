{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ad66a-756f-4d75-9603-54b16d6238ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integracion Texto + Variables estructuradas\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# --- Paso 2: Separar variable objetivo y datos ---\n",
    "\n",
    "y = df_reducido['target']                  # Variable objetivo\n",
    "X = df_reducido.drop(columns=['target'])  # Datos para modelo\n",
    "\n",
    "# --- Paso 3: Definir columnas por tipo para la transformación ---\n",
    "\n",
    "col_texto = 'clean_text'\n",
    "columnas_numericas = ['bathrooms_text', 'price', 'number_of_reviews', 'review_scores_rating']\n",
    "columnas_categoricas = ['host_is_superhost', 'property_type', 'room_type']\n",
    "\n",
    "# --- Paso 4: Importar y crear transformadores ---\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=500)\n",
    "scaler = StandardScaler()\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "preprocesador = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('texto', tfidf, col_texto),\n",
    "        ('num', scaler, columnas_numericas),\n",
    "        ('cat', ohe, columnas_categoricas)\n",
    "    ],\n",
    "    remainder='drop'  # eliminar columnas no listadas\n",
    ")\n",
    "\n",
    "# --- Paso 5: Aplicar transformación ---\n",
    "\n",
    "X_final = preprocesador.fit_transform(X)\n",
    "\n",
    "# --- Paso 6: Resultados ---\n",
    "\n",
    "print(\"Tipo de X_final:\", type(X_final))\n",
    "print(\"Forma de X_final:\", X_final.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4321a2-3581-48e3-88e8-0a93a680a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de categorías nuevas que te dio el error, seccion de entrenamiento\n",
    "categorias_nuevas = ['Holiday park', 'Shared room in casa particular']\n",
    "\n",
    "conteo = df_final[df_final['property_type'].isin(categorias_nuevas)].shape[0]\n",
    "porcentaje = conteo / len(df_final) * 100\n",
    "\n",
    "print(f\"Filas con categorías nuevas en 'property_type': {conteo}\")\n",
    "print(f\"Porcentaje sobre total: {porcentaje:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64322438-c8e3-4996-b65f-19b7bfd45d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo = df_fc['target'].value_counts()\n",
    "print (conteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f910e56-bfd5-4aaf-a58e-5bd3aa53970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carga de librerías\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt', download_dir='/Users/aaronmelamed/nltk_data') # las librerias locales nltk se guardan en la carpeta nltk_data local\n",
    "nltk.download('wordnet', download_dir='/Users/aaronmelamed/nltk_data')\n",
    "nltk.download('omw-1.4', download_dir='/Users/aaronmelamed/nltk_data')\n",
    "nltk.download('stopwords', download_dir='/Users/aaronmelamed/nltk_data')\n",
    "nltk.download('vader_lexicon', download_dir='/Users/aaronmelamed/nltk_data')\n",
    "nltk.data.path.append('/Users/aaronmelamed/nltk_data')\n",
    "\n",
    "# carga de datos listings = pd.read_csv('listings.csv')\n",
    "df_l = pd.read_csv('listings.csv')\n",
    "# carga de datos reviews = pd.read_csv('reviews.csv')\n",
    "df_r = pd.read_csv('reviews.csv')\n",
    "\n",
    "# 1. Formulación del problema\n",
    "# definir variables objetivo basadas en la calificación de los reviews\n",
    "df_l['review_scores_rating'] = df_l['review_scores_rating'].fillna(0)  # Rellenar NaN con 0\n",
    "df_l['review_scores_rating'] = df_l['review_scores_rating'].astype(float)  # Asegurar que sea float\n",
    "df_l = df_l[df_l['review_scores_rating'] > 0]  # Filtrar filas con calificación positiva\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "nltk.download('words', download_dir='/Users/aaronmelamed/nltk_data')\n",
    "english_vocab = set(w.lower() for w in words.words()) # elimina palabras que no están en el vocabulario inglés\n",
    "# Inicializar NLTK\n",
    "for pkg in ['stopwords', 'wordnet', 'omw-1.4']: # Descargar recursos necesarios de NLTK\n",
    "    nltk.download(pkg, quiet=True)\n",
    "# Definir la variable objetivo\n",
    "import nltk\n",
    "# Definir variable objetivo binaria: 1 = alta calificación (≥ 4.5), 0 = baja calificación (< 4.5)\n",
    "df_l['target'] = df_l['review_scores_rating'].apply(lambda x: 1 if x >= 4.5 else 0) # Verificar la creación de la variable objetivo\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(x='target', data=df_l, palette='Set2') # Gráfico de barras de la variable objetivo\n",
    "plt.title('#1 Distribución de la Variable Objetivo (target)')\n",
    "plt.xlabel('Clase (0 = baja, 1 = alta)')\n",
    "plt.ylabel('Cantidad de registros')\n",
    "plt.grid(axis='y')\n",
    "plt.savefig(\"/Users/aaronmelamed/Python 2025 Mineria de datos/output-py-final/#1_distribucion_variable_objetivo.png\")\n",
    "plt.close()\n",
    "# Mostrar una muestra de 20 filas con la calificación original y su clase binaria\n",
    "df_l[['review_scores_rating', 'target']].sample(20, random_state=1)\n",
    "\n",
    "# 2. Procesamiento de texto\n",
    "# limpiar el texto de la columna 'description'\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"Limpia el texto eliminando puntuación, convirtiendo a minúsculas y lematizando.\"\"\"\n",
    "    stop_words = set(stopwords.words('english')) # Usar stopwords en inglés\n",
    "    lemmatizer = WordNetLemmatizer() # Inicializar el lematizador\n",
    "    \n",
    "    # 2.2 Convertir a minúsculas\n",
    "    texto = texto.lower() # Convertir todo el texto a minúsculas\n",
    "    # 2.3  Eliminar puntuación\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation)) # Eliminar puntuación\n",
    "    # 2.3 (tokenizar) Eliminar stopwords y lematizar y filtrar por vocabulario inglés\n",
    "    tokens = texto.split() # Tokenizar el texto en palabras\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and w in english_vocab] # Eliminar stopwords, lematizar y filtrar por vocabulario inglés\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "# 2.1  aplicar la función de limpieza al texto de la columna 'description'\n",
    "df_l['description_clean'] = df_l['description'].astype(str).apply(limpiar_texto) # Verificar la limpieza del texto\n",
    "\n",
    "# 2.3 Usar TfidfVectorizer para vectorizar la columna 'description_clean'\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)  # Limitar a 100 características\n",
    "X = tfidf_vectorizer.fit_transform(df_l['description_clean'])\n",
    "print(f\"#2 Dimensiones de la matriz TF-IDF: {X.shape}\")  # Confirmar dimensiones\n",
    "df_tfidf = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out()) # Convertir la matriz TF-IDF a un DataFrame\n",
    "print(df_tfidf.head(10)) # Mostrar las primeras 10 filas del DataFrame resultante\n",
    "\n",
    "# 2.4 crear visualizaiones como WordCloudsbde frecuencias por clase\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "def generar_wordcloud(texto, titulo): #\n",
    "    \"\"\"Genera y guarda una WordCloud a partir del texto proporcionado.\"\"\"\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto) # Generar la WordCloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(titulo)\n",
    "    nombre_archivo = titulo.replace(' ', '_').replace('#', '')\n",
    "    ruta = f\"/Users/aaronmelamed/Python 2025 Mineria de datos/output-py-final/{nombre_archivo}.png\"\n",
    "    plt.savefig(ruta)\n",
    "    plt.close()\n",
    "# Generar WordClouds para las descripciones de alta y baja calificación\n",
    "textos_altos = ' '.join(df_l[df_l['target'] == 1]['description_clean'])\n",
    "textos_bajos = ' '.join(df_l[df_l['target'] == 0]['description_clean'])\n",
    "generar_wordcloud(textos_altos, 'WordCloud - Alta Calificación') # Verificar la generación de la WordCloud\n",
    "generar_wordcloud(textos_bajos, 'WordCloud - Baja Calificación') \n",
    "print(\"#2 WordClouds generadas para alta y baja calificación.\")\n",
    "# Generar histogramas de frecuencias de palabras\n",
    "def generar_histograma_frecuencias(texto, titulo):\n",
    "    \"\"\"Genera y muestra un histograma de frecuencias de palabras.\"\"\"\n",
    "    from collections import Counter\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    palabras = texto.split() # Tokenizar el texto en palabras\n",
    "    frecuencias = Counter(palabras) # Contar la frecuencia de cada palabra\n",
    "    palabras_comunes = frecuencias.most_common(20)  # Top 20 palabras\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(*zip(*palabras_comunes))\n",
    "    plt.title(titulo)\n",
    "    plt.xlabel('Palabras')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.xticks(rotation=45)\n",
    "    ruta = f\"/Users/aaronmelamed/Python 2025 Mineria de datos/output-py-final/#2_{titulo.replace(' ', '_')}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ruta)\n",
    "    plt.close()\n",
    "# Generar histogramas de frecuencias para alta y baja calificación\n",
    "generar_histograma_frecuencias(textos_altos, 'Histograma - Alta Calificación')\n",
    "generar_histograma_frecuencias(textos_bajos, 'Histograma - Baja Calificación')\n",
    "\n",
    "# 2.5 Analisis de sentimiento\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # Inicializar el analizador de sentimientos\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def analizar_sentimiento(texto): # Función para analizar el sentimiento de un texto\n",
    "    \"\"\"Analiza el sentimiento del texto y devuelve un puntaje compuesto.\"\"\"\n",
    "    return sia.polarity_scores(texto)['compound']\n",
    "df_l['sentiment_score'] = df_l['description_clean'].apply(analizar_sentimiento) # Aplicar el análisis de sentimiento a la columna 'description_clean'\n",
    "print(df_l[['description_clean', 'sentiment_score']].head(20)) # Mostrar las primeras 10 filas con el puntaje de sentimiento\n",
    "\n",
    "# 3. Preprocesamiento de Datos Estructurados\n",
    "# 3.1 Seleccionar columnas relevantes\n",
    "columnas_relevantes = ['room_type', 'host_is_superhost', 'price']\n",
    "df_l = df_l[columnas_relevantes + ['target', 'review_scores_rating', 'description_clean', 'sentiment_score']]  # si necesitas otras\n",
    "\n",
    "# 3.2 Tratar valores nulos\n",
    "df_l['room_type'] = df_l['room_type'].fillna('Unknown') # asumir 'Unknown' como valor por defecto\n",
    "df_l['host_is_superhost'] = df_l['host_is_superhost'].fillna('f')  # asumir 'f' como valor por defecto\n",
    "\n",
    "# 3.3 Codificar variables categóricas (usamos solo una técnica)\n",
    "df_l = pd.get_dummies(df_l, columns=['room_type', 'host_is_superhost'], drop_first=True) # # Convertir variables categóricas en variables dummy\n",
    "\n",
    "# 3.4 Normalizar valores numéricos\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Limpiar el campo 'price' eliminando símbolos y convirtiendo a float\n",
    "df_l['price'] = df_l['price'].replace('[\\$,]', '', regex=True) # # eliminar símbolos de dólar y comas\n",
    "df_l['price'] = df_l['price'].str.replace(',', '', regex=False)  # elimina comas si hay separadores de miles\n",
    "df_l['price'] = df_l['price'].astype(float)\n",
    "scaler = MinMaxScaler()\n",
    "df_l[['price', 'review_scores_rating', 'sentiment_score']] = scaler.fit_transform(df_l[['price', 'review_scores_rating', 'sentiment_score']])\n",
    "\n",
    "# Mostrar las primeras 20 filas\n",
    "print(df_l.head(20))\n",
    "\n",
    "# 4. Integración Texto + Variables Estructuradas\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Asegurar que 'description_clean' esté limpia y sea texto\n",
    "df_l['description_clean'] = df_l['description_clean'].fillna('').astype(str)\n",
    "\n",
    "# Asegurar que 'price' sea numérica\n",
    "df_l['price'] = df_l['price'].astype(str) # convertir a string para limpieza\n",
    "df_l['price'] = df_l['price'].replace(r'[\\$,]', '', regex=True) # # eliminar símbolos de dólar\n",
    "df_l['price'] = df_l['price'].str.replace(',', '', regex=False) # eliminar comas si hay separadores de miles\n",
    "df_l['price'] = df_l['price'].astype(float) # convertir a float\n",
    "\n",
    "# Escalar numéricos\n",
    "scaler = MinMaxScaler()\n",
    "df_l[['price', 'review_scores_rating', 'sentiment_score']] = scaler.fit_transform(\n",
    "    df_l[['price', 'review_scores_rating', 'sentiment_score']] \n",
    ") # Normalizar precios y puntuaciones\n",
    "\n",
    "df_l = df_l.dropna() # Eliminar filas con valores faltantes antes del entrenamiento\n",
    "\n",
    "# Variables estructuradas\n",
    "columnas_estructuradas = ['room_type_Private room', 'room_type_Shared room', 'host_is_superhost_t', 'price', 'review_scores_rating', 'sentiment_score']\n",
    "\n",
    "# Definir X e y\n",
    "X = df_l.drop(columns=['target'])\n",
    "y = df_l['target']\n",
    "\n",
    "### Construir pipeline de preprocesamiento ###\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', TfidfVectorizer(max_features=100), 'description_clean'),\n",
    "        ('struct', 'passthrough', columnas_estructuradas)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "### Crear pipeline completo con modelo ###\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Separar en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predecir y evaluar\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 5. Modelado Predictivo:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Definir X e y\n",
    "X = df_l.drop(columns=['target'])\n",
    "y = df_l['target']\n",
    "\n",
    "# Asegurar tipos correctos de categóricas\n",
    "cat_cols = ['room_type_Private room', 'room_type_Shared room', 'host_is_superhost_t']\n",
    "X[cat_cols] = X[cat_cols].astype(str)\n",
    "\n",
    "# Separar en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 5.1  Entrenar con Pipeline (requisito 2) ===\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(max_features=100), 'description_clean'),\n",
    "        ('cat', OneHotEncoder(), cat_cols),\n",
    "        ('num', StandardScaler(), ['price', 'review_scores_rating', 'sentiment_score'])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ") # Definir preprocesador para texto, categóricas y numéricas\n",
    "\n",
    "# 5.2 Aplicar al menos dos modelos supervisados (requisito 1) ===\n",
    "modelos = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=500, random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "} # Definir modelos a evaluar\n",
    "\n",
    "# Entrenar y evaluar modelos\n",
    "for nombre, modelo in modelos.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', modelo)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    print(f\"\\n RESULTADOS: {nombre} ===\")\n",
    "    print(\"Matriz de Confusión:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"Reporte de Clasificación:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Guardar el modelo\n",
    "    joblib.dump(pipeline, f'modelo_airbnb_{nombre}.pkl')\n",
    "\n",
    "# 5.3 Optimizar hiperparámetros con GridSearchCV (requisito 3) ===\n",
    "parametros = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "} # Definir parámetros para RandomForestClassifier\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "]) # Crear pipeline para RandomForestClassifier\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_rf,\n",
    "    param_grid=parametros,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ") # Ajustar GridSearchCV\n",
    "grid_search.fit(X_train, y_train) # Entrenar el modelo con GridSearchCV\n",
    "\n",
    "# Evaluar mejor modelo\n",
    "y_pred_mejor = grid_search.predict(X_test)\n",
    "print(\"\\n 5. RESULTADOS DEL MEJOR MODELO\")\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_pred_mejor))\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred_mejor))\n",
    "print(\"Mejores parámetros:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Guardar el mejor modelo encontrado\n",
    "joblib.dump(grid_search.best_estimator_, 'mejor_modelo_airbnb.pkl')\n",
    "\n",
    "# 6. Evaluación del Modelo:\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el mejor modelo\n",
    "mejor_modelo = joblib.load('mejor_modelo_airbnb.pkl')\n",
    "\n",
    "# Predecir\n",
    "y_pred_final = mejor_modelo.predict(X_test)\n",
    "y_pred_proba = mejor_modelo.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 6.1 MÉTRICAS CLÁSICAS ===\n",
    "print(\"\\n=== EVALUACIÓN DEL MEJOR MODELO ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_final))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_final))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_final))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_final))\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_pred_final))\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred_final))\n",
    "\n",
    "# CURVA ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC - Modelo Final')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/Users/aaronmelamed/Python 2025 Mineria de datos/output-py-final/#6_curva_roc_modelo_final.png\")\n",
    "plt.close()\n",
    "\n",
    "# 6.2 IMPORTANCIA DE FEATURES\n",
    "feature_names = mejor_modelo.named_steps['preprocessor'].get_feature_names_out()\n",
    "importancias = mejor_modelo.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Mostrar las 10 características más importantes\n",
    "importancia_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importancias\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"\\n=== FEATURES MÁS IMPORTANTES ===\")\n",
    "print(importancia_df.head(10))\n",
    "\n",
    "# Guardar modelo final\n",
    "joblib.dump(mejor_modelo, 'modelo_final_airbnb.pkl')\n",
    "print(\"\\nModelo final guardado como 'modelo_final_airbnb.pkl'.\")\n",
    "print(\"Análisis completado y modelos guardados exitosamente.\")\n",
    "\n",
    "# 7. Visualización de Resultados\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "# 7.1 WordClouds por categoría de rating ===\n",
    "textos_altos = ' '.join(df_l[df_l['target'] == 1]['description_clean'])\n",
    "textos_bajos = ' '.join(df_l[df_l['target'] == 0]['description_clean'])\n",
    "\n",
    "generar_wordcloud(textos_altos, '#7 WordCloud - Alta Calificación')\n",
    "generar_wordcloud(textos_bajos, '#7 WordCloud - Baja Calificación')\n",
    "\n",
    "# 7.2 Mapa de calor de la distribución de precios ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    df_l.pivot_table(index='room_type_Private room', columns='target', values='price', aggfunc='mean'),\n",
    "    annot=True, cmap='coolwarm'\n",
    ")\n",
    "plt.title('Mapa de Calor - Distribución de Precios por Tipo de Habitación y Calificación')\n",
    "plt.xlabel('Calificación (0 = Baja, 1 = Alta)')\n",
    "plt.ylabel('Tipo de Habitación')\n",
    "plt.savefig(\"/Users/aaronmelamed/Python 2025 Mineria de datos/output-py-final/#7_mapa_calor_precios_por_tipo_y_calificacion.png\")\n",
    "plt.close()\n",
    "\n",
    "# 7.3 Matriz de confusión del mejor modelo ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_mejor), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de Confusión - Mejor Modelo')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Realidad')\n",
    "plt.savefig(\"/Users/aaronmelamed/Python 2025 Mineria de datos/output-py-final/#7_matriz_confusion_mejor_modelo.png\")\n",
    "plt.close()\n",
    "\n",
    "# 7.4 SHAP clásico – Gráfico de importancia de características\n",
    "import shap\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "# Extraer pasos del modelo\n",
    "preprocessor = mejor_modelo.named_steps['preprocessor']\n",
    "modelo_rf = mejor_modelo.named_steps['classifier']\n",
    "\n",
    "# Transformar datos\n",
    "X_transformado = preprocessor.transform(X_test)\n",
    "if scipy.sparse.issparse(X_transformado):\n",
    "    X_transformado = X_transformado.toarray()\n",
    "X_transformado = X_transformado.astype('float64')\n",
    "\n",
    "# Crear el explicador SHAP y calcular valores\n",
    "explainer = shap.TreeExplainer(modelo_rf)\n",
    "shap_values = explainer.shap_values(X_transformado)\n",
    "\n",
    "import numpy as np\n",
    "# Validar forma de salida\n",
    "# Si shap_values tiene 3 dimensiones, extraer solo la clase positiva (índice 1)\n",
    "if isinstance(shap_values, np.ndarray) and shap_values.ndim == 3:\n",
    "    shap_vals = shap_values[:, :, 1]  # clase positiva\n",
    "elif isinstance(shap_values, list):\n",
    "    shap_vals = shap_values[1] if len(shap_values) > 1 else shap_values[0]\n",
    "else:\n",
    "    shap_vals = shap_values\n",
    "\n",
    "print(f\"X_transformado.shape: {X_transformado.shape}\")\n",
    "print(f\"shap_vals.shape: {shap_vals.shape}\")\n",
    "\n",
    "# Obtener nombres de características\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Crear gráfico resumen SHAP\n",
    "shap.summary_plot(shap_vals, X_transformado, feature_names=feature_names, show=False)\n",
    "plt.title(\"Importancia de Características según SHAP\")\n",
    "plt.savefig(\"/Users/aaronmelamed/Python 2025 Mineria de datos/output-py-final/#7_importancia_features_SHAP.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# 7.5 SHAP explicativo sobre texto limpio (modo clásico para texto)\n",
    "# Generar explicador directamente desde modelo final sobre pipeline de texto\n",
    "\n",
    "# Crear un nuevo pipeline sólo para texto\n",
    "text_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=100)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "# Ajustar a texto únicamente\n",
    "text_pipeline.fit(df_l['description_clean'], df_l['target'])\n",
    "\n",
    "# Crear explicador SHAP clásico para texto\n",
    "explainer_text = shap.Explainer(\n",
    "    text_pipeline.named_steps['classifier'],\n",
    "    text_pipeline.named_steps['tfidf'].transform(df_l['description_clean']).toarray()\n",
    ")\n",
    "textos_muestra = [\n",
    "    \"beautiful apartment with excellent view\",\n",
    "    \"dirty room and poor customer service\",\n",
    "    \"quiet and cozy place near the city center\"\n",
    "]\n",
    "shap_vals_text = explainer_text(\n",
    "    text_pipeline.named_steps['tfidf'].transform(textos_muestra).toarray()\n",
    ")\n",
    "\n",
    "# Mostrar explicaciones tipo texto (visualización estándar por importancia)\n",
    "shap.summary_plot(\n",
    "    shap_vals_text.values,\n",
    "    features=text_pipeline.named_steps['tfidf'].transform(textos_muestra).toarray(),\n",
    "    feature_names=text_pipeline.named_steps['tfidf'].get_feature_names_out(),\n",
    "    show=False\n",
    ")\n",
    "plt.savefig(\"/Users/aaronmelamed/Python 2025 Mineria de datos/output-py-final/#7_shap_texto_limpo.png\", bbox_inches='tight')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
