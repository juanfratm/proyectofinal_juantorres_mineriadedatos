{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7921fe6-b0db-405b-bfad-f9ae21c51819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtro para el idioma que predomina en el dataset\n",
    "\n",
    "#Primero hacemos una exploracion rapida de la columna Comments para ver la tendencia de idiomas.\n",
    "#debido a la cantidad de datos  se toma una muestra aleatorea de 10000\n",
    "\n",
    "import pandas as pd\n",
    "import re \n",
    "import os  \n",
    "import emoji\n",
    "import glob\n",
    "from langdetect import detect, detect_langs, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7f2eba-09e2-4829-b4a4-cb2269a27fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para detectar idioma \n",
    "def detectar_idioma(texto):\n",
    "    try:\n",
    "        return detect(str(texto))\n",
    "    except:\n",
    "        return \"desconocido\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afea7d73-5bcd-4ae6-923f-473061f089a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una lista para guardar los idiomas que se detecten\n",
    "idiomas_detectados = []\n",
    "datos = pd.read_csv('Datos_Completos_finales.csv', usecols=['comments'], chunksize=100000) # Seleccionamos una parte del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b4542cf-3125-450e-8a62-0aec63cd32ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ciclo for para leer archivo por partes \n",
    "for bloque in datos:\n",
    "    # Filtrar solo comentarios no nulos\n",
    "    bloque = bloque.dropna(subset=['comments'])\n",
    "\n",
    "    # Tomamos una pequeña muestra del bloque (10000 filas)\n",
    "    muestra = bloque.sample(n=10000, random_state=42)\n",
    "\n",
    "    # Usamos la funcion para detectar el idioma en la muestra\n",
    "    muestra['idioma'] = muestra['comments'].apply(detectar_idioma)\n",
    "\n",
    "    # Agregamos los idiomas detectados a la lista creada arriba\n",
    "    idiomas_detectados.extend(muestra['idioma'].tolist())\n",
    "\n",
    "    # Detenemos después de cierto número de muestras\n",
    "    if len(idiomas_detectados) >= 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135adee3-b04e-4fc7-a6f9-798623cc67a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idiomas detectados:\n",
      "idioma\n",
      "en             5649\n",
      "es             3864\n",
      "fr              116\n",
      "pt               91\n",
      "ro               57\n",
      "de               47\n",
      "desconocido      28\n",
      "it               22\n",
      "zh-cn            15\n",
      "ca               11\n",
      "ru               11\n",
      "id               11\n",
      "so               11\n",
      "ko               10\n",
      "af                7\n",
      "cy                6\n",
      "nl                6\n",
      "tl                5\n",
      "sk                5\n",
      "no                4\n",
      "cs                4\n",
      "da                3\n",
      "ja                3\n",
      "he                2\n",
      "zh-tw             2\n",
      "tr                2\n",
      "vi                1\n",
      "hr                1\n",
      "fi                1\n",
      "sv                1\n",
      "lt                1\n",
      "sl                1\n",
      "et                1\n",
      "el                1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# creamos un DataFrame con los idiomas detectados\n",
    "df_idiomas = pd.DataFrame(idiomas_detectados, columns=['idioma'])\n",
    "\n",
    "# colocamos un contador de idiomas detectados\n",
    "conteo = df_idiomas['idioma'].value_counts()\n",
    "\n",
    "# Mostrarmos resultados\n",
    "print(\"Idiomas detectados:\")\n",
    "print(conteo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d397f35d-21d1-43af-b28a-f4dedc00a91c",
   "metadata": {},
   "source": [
    "Vemos que el idioma predominante es el Ingles, por lo que vamos a proceder a filtrar el texto de comments en ese idioma (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dcc4650-3a11-42c0-b4f6-ac0bc3933657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrado del idioma predominante\n",
    "\n",
    "# Se fija seed aleatoria para garantizar reproducibilidad en los resultados de la función de detección de idioma\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31fa3bda-0067-46d3-a582-32e8f66839dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una funcion para limpiar el texto de emojis, de etiquetas html, espacios, simbolos y signos especiales\n",
    "#Realizamos esto para facilitar la deteccion del idioma, no eliminamos las puntuaciones basicas.\n",
    "def limpiar_texto(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        # Si el texto no es string, devolvemos cadena vacía para evitar errores\n",
    "        return \"\"\n",
    "    # Eliminamos emojis modernos\n",
    "    texto = emoji.replace_emoji(texto, replace='')\n",
    "\n",
    "    # Eliminamos emojis unicode extendidos con expresión regular\n",
    "    texto = re.sub(\n",
    "        \"[\" \n",
    "        \"\\U0001F600-\\U0001F64F\"  # Caritas\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Símbolos y pictogramas\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transporte y símbolos varios\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # Banderas\n",
    "        \"\\U00002700-\\U000027BF\"  # Flechas, corazones y otros\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Emojis recientes\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Más emojis modernos\n",
    "        \"\\U00002600-\\U000026FF\"  # Símbolos varios\n",
    "        \"\\U000024C2-\\U0001F251\"  # Otros símbolos\n",
    "        \"]+\", \"\", texto)\n",
    "\n",
    "    # Eliminamos emoticones de texto comunes como :) :( :D :-P etc.\n",
    "    texto = re.sub(r'(:|;|=|8|x|X)[\\-~]?[)(DPpOo3/\\\\|]+', '', texto)\n",
    "\n",
    "    # Eliminamos patrones de caracteres repetidos tipo ^^^ o ---  que son innecesarios\n",
    "    texto = re.sub(r'[\\^><\\-_=]{2,}', '', texto)\n",
    "\n",
    "    # Reemplazamos las etiquetas HTML específicas <br> y <br/> por espacio\n",
    "    texto = re.sub(r'<br\\s*/?>', ' ', texto, flags=re.IGNORECASE)\n",
    "\n",
    "    # Eliminamos otras etiquetas HTML generales <i>, <b>, etc.\n",
    "    texto = re.sub(r'<[^>]+>', '', texto)\n",
    "\n",
    "    # Eliminamos símbolos especiales\n",
    "    texto = re.sub(r'[{}\\[\\]*#^|~=`+<>]', '', texto)\n",
    "\n",
    "    # Reemplazamos múltiples espacios seguidos por uno solo\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    return texto.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "238315cb-4b23-4c39-892f-2fb2ba201866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora creamos una funcion para filtrar el idioma predominante o escogido\n",
    "def ingles(texto_limpio):\n",
    "    try:\n",
    "        # Detecta idiomas y sus probabilidades (usamos probabilidades porque al revisar un poco del dataset observamos que hay comentarios en spanglish)\n",
    "        idiomas = detect_langs(texto_limpio)\n",
    "\n",
    "        # Si hay dos idiomas y el segundo es mayor a 10%, no lo usamos porque está mezclado\n",
    "        if len(idiomas) > 1 and idiomas[1].prob > 0.10:\n",
    "            return False  # Texto mezclado, no se acepta\n",
    "\n",
    "        ## Si no es inglés o no estamos seguros (menos del 90%), lo rechazamos\n",
    "        if idiomas[0].lang != 'en' or idiomas[0].prob < 0.90:\n",
    "            return False\n",
    "\n",
    "        # Lista básica de palabras comunes en español para detectar mezcla, esto ayuda al detector\n",
    "        palabras_es = [\n",
    "            'muy', 'bueno', 'lugar', 'bonito', 'excelente',\n",
    "            'calle', 'comida', 'habitacion', 'gracias', 'centro',\n",
    "            'cerca', 'todo', 'nada', 'persona', 'amiga'\n",
    "        ]\n",
    "\n",
    "        # Recorremos palabras en español para verificar si aparecen en el texto\n",
    "        for palabra in palabras_es:\n",
    "            # Buscamos palabra como palabra completa\n",
    "            if f\" {palabra} \" in f\" {texto_limpio.lower()} \":\n",
    "                return False  # Si contiene palabra en español, descartamos\n",
    "\n",
    "        # Si pasa todas las pruebas, devolvemos True\n",
    "        return True\n",
    "\n",
    "    except LangDetectException:\n",
    "        # Si no pudo detectar el idioma, rechazamos el texto\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d4038a1-4457-4256-b2cd-ee4500512982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora que tenemos las funciones las aplicaremos al dataset previamnete generado en elcodigo principal\n",
    "#Aqui tambien vamos a analizar cada 100mil filas ya que sino realizamos la particion la computadora se congela\n",
    "archivo_csv = 'Datos_Completos_finales.csv' \n",
    "columna_texto = 'comments'  \n",
    "tamaño_batch = 100000  \n",
    "carpeta_salida = 'Datos_filtrados_EN'  \n",
    "\n",
    "# Creamos la carpeta de salida para que no se mezclen los dataset, ya que son mas de 1 millon de datos para analizar, saldran al menos 10 batch\n",
    "os.makedirs(carpeta_salida, exist_ok=True)\n",
    "\n",
    "# Contador de batchs procesados\n",
    "batch_num = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a140b19-d02d-42bf-a7ae-3e1a223089d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando batch 1...\n",
      "Batch 1 guardado con 53962 comentarios en inglés.\n",
      "\n",
      "Procesando batch 2...\n",
      "Batch 2 guardado con 43155 comentarios en inglés.\n",
      "\n",
      "Procesando batch 3...\n",
      "Batch 3 guardado con 40212 comentarios en inglés.\n",
      "\n",
      "Procesando batch 4...\n",
      "Batch 4 guardado con 33467 comentarios en inglés.\n",
      "\n",
      "Procesando batch 5...\n",
      "Batch 5 guardado con 30355 comentarios en inglés.\n",
      "\n",
      "Procesando batch 6...\n",
      "Batch 6 guardado con 24874 comentarios en inglés.\n",
      "\n",
      "Procesando batch 7...\n",
      "Batch 7 guardado con 29966 comentarios en inglés.\n",
      "\n",
      "Procesando batch 8...\n",
      "Batch 8 guardado con 31270 comentarios en inglés.\n",
      "\n",
      "Procesando batch 9...\n",
      "Batch 9 guardado con 31725 comentarios en inglés.\n",
      "\n",
      "Procesando batch 10...\n",
      "Batch 10 guardado con 29739 comentarios en inglés.\n",
      "\n",
      "Procesando batch 11...\n",
      "Batch 11 guardado con 28334 comentarios en inglés.\n",
      "\n",
      "Procesando batch 12...\n",
      "Batch 12 guardado con 26100 comentarios en inglés.\n",
      "\n",
      "Procesando batch 13...\n",
      "Batch 13 guardado con 1889 comentarios en inglés.\n"
     ]
    }
   ],
   "source": [
    "# Ciclo for para procesar el dataframe por batch y filtrar los comentarios solo en ingles\n",
    "for chunk in pd.read_csv(archivo_csv, chunksize=tamaño_batch):\n",
    "    print(f\"\\nProcesando batch {batch_num + 1}...\")\n",
    "\n",
    "    # Lista para almacenar filas filtradas\n",
    "    comentarios_en = []  \n",
    "\n",
    "    # Recorremos cada fila del batch actual\n",
    "    for _, fila in chunk.iterrows():\n",
    "        texto_original = str(fila[columna_texto])  # para obtener texto original\n",
    "        texto_limpio = limpiar_texto(texto_original)  # para limpiar el texto\n",
    "\n",
    "        # Ignoramos textos demasiado cortos para evitar ruido\n",
    "        if len(texto_limpio) < 20:\n",
    "            continue\n",
    "\n",
    "        # Verificar si el texto esta inglés según la función que hemos creado\n",
    "        if ingles(texto_limpio):\n",
    "            # Copiamos la fila para no modificar el original\n",
    "            fila_filtrada = fila.copy()\n",
    "            # Agregamos una columna extra con el texto limpio\n",
    "            fila_filtrada['comentario_limpio'] = texto_limpio\n",
    "            # Guardarmos la fila en lista de resultados\n",
    "            comentarios_en.append(fila_filtrada)\n",
    "\n",
    "    # Convertimos la lista de filas filtradas a DataFrame\n",
    "    df_filtrado = pd.DataFrame(comentarios_en)\n",
    "\n",
    "    #Nombre de los archivo de salida según el batch\n",
    "    nombre_archivo = f'{carpeta_salida}/comentarios_en_batch_{batch_num}.csv'\n",
    "\n",
    "    # Guardamos cada DataFrame\n",
    "    df_filtrado.to_csv(nombre_archivo, index=False)\n",
    "    print(f\"Batch {batch_num + 1} guardado con {len(df_filtrado)} comentarios en inglés.\")\n",
    "    \n",
    "    # Incrementarmos el contador para siguiente batch\n",
    "    batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adce5d1a-cfbb-4c7a-b713-7e2b2b3786cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo final guardado como 'Archivo_Filtrado_EN.csv' con 405048 comentarios.\n"
     ]
    }
   ],
   "source": [
    "#ahora que se han generado los batch (proceso demora aprox 2horas en computadoras regulares) debemos hacerlo que sea un solo dataset para continuar con el proyecto\n",
    "# Seleccionamos la ruta de entrada (carpeta donde están los batches)\n",
    "carpeta_batches = 'Datos_filtrados_EN'\n",
    "\n",
    "# Buscar todos los archivos CSV que empiecen con comentarios_en_batch tal\n",
    "archivos = glob.glob(os.path.join(carpeta_batches, 'comentarios_en_batch_*.csv'))\n",
    "\n",
    "# Leemos todos los archivos en una lista de dataframes\n",
    "dataframes = [pd.read_csv(archivo) for archivo in archivos]\n",
    "\n",
    "# Unimos todos los dataframes\n",
    "df_final = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "#Ruta de salida(fuera de la carpeta de datos_filtrados_en), colocamos el archivo donde esta el codigo 1\n",
    "archivo_salida = 'Archivo_Filtrado_EN.csv'\n",
    "df_final.to_csv(archivo_salida, index=False)\n",
    "\n",
    "print(f\"Archivo final guardado como '{archivo_salida}' con {len(df_final)} comentarios.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937f232-a184-4e9a-af2f-9519d3cca137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
